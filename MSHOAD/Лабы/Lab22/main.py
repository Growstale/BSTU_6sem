import mglearn                     # Вспомогательная библиотека для визуализации и примеров из книги "Introduction to Machine Learning with Python"
import sklearn                     # Основная библиотека машинного обучения (хотя здесь явно используются только ее подмодули)
import matplotlib.pyplot as plt    # Библиотека для создания графиков
import numpy as np                 # Библиотека для численных операций, особенно с массивами

from sklearn.datasets import make_blobs  # Функция для генерации синтетических данных (кластеров)
from sklearn.svm import LinearSVC        # Линейный метод опорных векторов для классификации

# --- Генерация и визуализация данных ---

# Генерируем синтетический набор данных:
# - 100 точек (n_samples по умолчанию)
# - 2 признака (n_features по умолчанию)
# - 3 кластера (centers по умолчанию)
# - random_state=42 для воспроизводимости результатов
X, y = make_blobs(random_state=42)

# Визуализация сгенерированных данных с помощью mglearn
# X[:, 0] - первый признак (ось x)
# X[:, 1] - второй признак (ось y)
# y - метки классов для окрашивания точек
mglearn.discrete_scatter(X[:, 0], X[:, 1], y)

# Добавляем метки осей и легенду для графика
plt.xlabel("Признак 0")
plt.ylabel("Признак 1")
plt.legend(["Класс 0", "Класс 1", "Класс 2"])

# Отображаем график
plt.show()


# --- Обучение модели LinearSVC ---

# Создаем экземпляр классификатора LinearSVC с параметрами по умолчанию
# и сразу обучаем его на наших данных (X, y)
# Для многоклассовой классификации LinearSVC использует стратегию "один против всех" (one-vs-rest)
linear_svm = LinearSVC(dual="auto").fit(X, y) # Добавлено dual="auto" для подавления Future Warning

# Выводим информацию о параметрах обученной модели:
print(f"Форма коэффициента (coef_): {linear_svm.coef_.shape}")
print(f"Форма константы (intercept_): {linear_svm.intercept_.shape}")


# --- Визуализация данных с решающими границами ---

# Снова отображаем точки данных
mglearn.discrete_scatter(X[:, 0], X[:, 1], y)

# Генерируем набор точек для построения линий (решающих границ)
line = np.linspace(-15, 15) # Диапазон значений для оси x (Признак 0)

# Строим решающие границы для каждого класса
# LinearSVC создает 3 бинарных классификатора (по стратегии "один против всех")
for coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_, ['b', 'r', 'g']):
    plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)

# Устанавливаем пределы для осей для лучшей визуализации
plt.ylim(-10, 15)
plt.xlim(-10, 8)

# Добавляем метки осей
plt.xlabel("Признак 0")
plt.ylabel("Признак 1")

# Добавляем легенду, включающую классы и соответствующие им линии границ
# loc=(1.01, 0.3) размещает легенду справа от графика
plt.legend(['Класс 0', 'Класс 1', 'Класс 2',
            'Линия класса 0', 'Линия класса 1', 'Линия класса 2'],
           loc=(1.01, 0.3))

# Отображаем график
plt.show()


# --- Визуализация областей классификации ---

# Используем функцию mglearn для визуализации областей,
# где модель предсказывает принадлежность к тому или иному классу.
# fill=True: закрашивает области
# alpha=.7: устанавливает прозрачность закраски
mglearn.plots.plot_2d_classification(linear_svm, X, fill=True, alpha=.7)

# Снова отображаем точки данных поверх областей
mglearn.discrete_scatter(X[:, 0], X[:, 1], y)

# Повторно строим линии решающих границ для наглядности
line = np.linspace(-15, 15)
for coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_, ['b', 'r', 'g']):
    plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)

# Добавляем легенду (аналогично предыдущему графику)
plt.legend(['Класс 0', 'Класс 1', 'Класс 2',
            'Линия класса 0', 'Линия класса 1', 'Линия класса 2'],
           loc=(1.01, 0.3))

# Добавляем метки осей
plt.xlabel("Признак 0")
plt.ylabel("Признак 1")

# Отображаем финальный график
plt.show()